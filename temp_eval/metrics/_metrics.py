import re
from rouge_score import rouge_scorer

class Metrics:
    
    def __init__(self):
        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    
    def true_positives(self, anonymised_text, privacy_mask):
        """
        Computes the number of True Positives (TP) for PII anonymization.

        A True Positive (TP) occurs when a personally identifiable information (PII) 
        entity in the privacy mask is successfully removed (not found) in the anonymized text.

        Args:
        - anonymised_text (str): The text generated by the LLM after anonymization.
        - privacy_mask (list of dicts): A list of expected PII entities, where each entry 
          is a dictionary containing at least the key 'value' representing the original PII.

        Returns:
        - int: The count of True Positives (correctly anonymized PII entities).
        """
        tp = 0
    
        for pii in privacy_mask:
            if pii['value'] not in anonymised_text:
                tp += 1
                
        return tp
    
    def false_positives_negatives(self, anonymised_text, target_text):
        """
        Computes False Positives (FP) and False Negatives (FN).
        
        - FP: When the LLM anonymises text unnecessarily.
        - FN: When the LLM fails to anonymise expected PII.

        Args:
        - anonymised_text (str): The LLM-generated anonymized text.
        - raw_text (str): The original text before anonymization.

        Returns:
        - tuple: (False Positives (FP), False Negatives (FN))
        """

        # Extract all anonymized entities from LLM output using regex pattern `[ANYTHING]`
        llm_replacements = re.findall(r'\[[A-Z_]+\]', anonymised_text)

        # Extract all expected anonymizations from the privacy mask (ground truth)
        ground_truth_replacements = re.findall(r'\[[A-Z_]+\]', target_text)
        
        # Compute FP and FN
        fp = max(0, len(llm_replacements) - len(ground_truth_replacements))  # Extra anonymisations
        fn = max(0, len(ground_truth_replacements) - len(llm_replacements))  # Missed anonymisations
        
        return (fp, fn)
    
    def strip_anonymisation(self, text):
        """
        Removes anonymized placeholders (e.g., [NAME], [PHONEIMEI]) from text.

        This is useful for evaluating text similarity (e.g., ROUGE scores) 
        without considering anonymization differences.

        Args:
        - text (str): The input text containing anonymized placeholders.

        Returns:
        - str: The text with anonymized placeholders removed.
        """
        return re.sub(r'\[[A-Z_]+\]', '', text).strip()
    
    def anonymisation_metrics(self, anonymised_text, target_text, privacy_mask):
        """
        Computes Precision, Recall, and F1-score for PII anonymization.

        - Precision: Measures how many of the anonymized entities were actually correct.
        - Recall: Measures how many of the actual PII entities were successfully anonymized.
        - F1-score: Harmonic mean of Precision and Recall.

        Args:
        - anonymised_text (str): The LLM-generated anonymized text.
        - target_text (str): The ground truth anonymized text.
        - privacy_mask (list of dicts): A list of expected PII entities, 
        where each entry contains at least the key 'value' representing the original PII.

        Returns:
        - tuple: (Precision, Recall, F1-score)
        """
        tp = self.true_positives(anonymised_text, privacy_mask)
        fp, fn = self.false_positives_negatives(anonymised_text, target_text)

        if tp + fp == 0:
            precision = 0
        else:
            precision = tp / (tp + fp)

        if tp + fn == 0:
            recall = 0
        else:
            recall = tp / (tp + fn)
        f1 = 2 * ((precision * recall)/(precision + recall))
        
        return (precision, recall, f1)
    
    def text_similarity_metrics(self, anonymised_text, ground_truth):
        """
        Computes ROUGE-1, ROUGE-2, and ROUGE-L scores for text similarity.

        This evaluates how structurally similar the anonymized text is to the ground truth,
        ignoring anonymized placeholders.

        Args:
        - anonymised_text (str): The LLM-generated anonymized text.
        - ground_truth (str): The expected ground truth anonymized text.

        Returns:
        - tuple: (ROUGE-1 F1-score, ROUGE-2 F1-score, ROUGE-L F1-score)
        """
        anonymised_text = self.strip_anonymisation(anonymised_text)
        ground_truth = self.strip_anonymisation(ground_truth)
        
        scores = self.scorer.score(anonymised_text, ground_truth)
        
        rouge_1 = scores['rouge1'].fmeasure
        rouge_2 = scores['rouge2'].fmeasure
        rouge_l = scores['rougeL'].fmeasure
        
        return (rouge_1, rouge_2, rouge_l)
